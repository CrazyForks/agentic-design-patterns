
# 附录 A：高级提示词技术

# 提示词简介

提示词（Prompting）是与语言模型交互的主要接口，是制定输入以引导模型生成期望输出的过程。这包括构建请求结构、提供相关上下文、指定输出格式以及展示预期的响应类型。设计良好的提示词可以最大限度地发挥语言模型的潜力，产生准确、相关和创造性的响应。相反，设计不当的提示词会导致模糊、不相关或错误的输出。

提示词工程的目标是始终如一地从语言模型中获得高质量的响应。这需要理解模型的能力和局限性，并有效地传达预期目标。它涉及通过学习如何最好地指导 AI 来发展与 AI 沟通的专业知识。

本附录详细介绍了超越基本交互方法的各种提示词技术。它探讨了构建复杂请求、增强模型推理能力、控制输出格式以及整合外部信息的方法论。这些技术适用于构建从简单聊天机器人到复杂多 Agent 系统的各种应用，可以提高 Agentic 应用的性能和可靠性。

Agentic 模式是构建智能系统的架构结构，在主要章节中有详细说明。这些模式定义了 Agent 如何规划、利用工具、管理内存和协作。这些 Agentic 系统的效能取决于它们与语言模型进行有意义交互的能力。

# 核心提示词原则

有效提示语言模型的核心原则：

有效的提示词建立在指导与语言模型沟通的基本原则之上，这些原则适用于各种模型和任务复杂度。掌握这些原则对于持续生成有用和准确的响应至关重要。

**清晰性和具体性**：指令应该明确无误且精确。语言模型解释模式；多种解释可能导致意外响应。定义任务、期望的输出格式以及任何限制或要求。避免模糊的语言或假设。不充分的提示词会产生模糊和不准确的响应，阻碍有意义的输出。

**简洁性**：虽然具体性至关重要，但不应损害简洁性。指令应该直接了当。不必要的措辞或复杂的句子结构可能会混淆模型或掩盖主要指令。提示词应该简单；对用户来说令人困惑的内容对模型来说也可能令人困惑。避免复杂的语言和多余的信息。使用直接的措辞和主动动词来清楚地描述期望的操作。有效的动词包括：Act（行动）、Analyze（分析）、Categorize（分类）、Classify（归类）、Contrast（对比）、Compare（比较）、Create（创建）、Describe（描述）、Define（定义）、Evaluate（评估）、Extract（提取）、Find（查找）、Generate（生成）、Identify（识别）、List（列出）、Measure（测量）、Organize（组织）、Parse（解析）、Pick（挑选）、Predict（预测）、Provide（提供）、Rank（排序）、Recommend（推荐）、Return（返回）、Retrieve（检索）、Rewrite（重写）、Select（选择）、Show（显示）、Sort（排序）、Summarize（总结）、Translate（翻译）、Write（编写）。

**使用动词**：动词选择是关键的提示词工具。动作动词指示预期的操作。与其说"Think about summarizing this"（考虑总结这个），不如使用直接的指令，如"Summarize the following text"（总结以下文本）更有效。精确的动词引导模型激活与特定任务相关的训练数据和过程。

**指令优于约束**：积极的指令通常比消极的约束更有效。指定期望的操作优于概述不应该做什么。虽然约束在安全性或严格格式化方面有其作用，但过度依赖可能导致模型专注于回避而不是目标。构建提示词以直接引导模型。积极的指令符合人类指导偏好并减少混淆。

**实验和迭代**：提示词工程是一个迭代过程。确定最有效的提示词需要多次尝试。从草稿开始，测试它，分析输出，识别不足，并改进提示词。模型变化、配置（如温度或 top-p）以及措辞的微小变化可能产生不同的结果。记录尝试对于学习和改进至关重要。实验和迭代是实现期望性能的必要条件。

这些原则构成了与语言模型有效沟通的基础。通过优先考虑清晰性、简洁性、动作动词、积极指令和迭代，建立了应用更高级提示词技术的强大框架。

# 基本提示词技术

基于核心原则，基础技术为语言模型提供不同级别的信息或示例来引导它们的响应。这些方法作为提示词工程的初始阶段，对广泛的应用有效。

## Zero-Shot 提示

Zero-shot 提示是最基本的提示形式，语言模型仅获得指令和输入数据，没有任何期望的输入-输出对示例。它完全依赖模型的预训练来理解任务并生成相关响应。本质上，zero-shot 提示由任务描述和开始过程的初始文本组成。

* **何时使用**：Zero-shot 提示对于模型在训练期间可能广泛遇到的任务通常足够，例如简单的问答、文本补全或对简单文本的基本总结。这是首先尝试的最快方法。
* **示例**：  
  Translate the following English sentence to French: 'Hello, how are you?'

## One-Shot 提示

One-shot 提示涉及在呈现实际任务之前，向语言模型提供单个输入及其对应期望输出的示例。这种方法作为初始演示，以说明模型预期复制的模式。目的是为模型提供一个具体实例，它可以用作模板来有效执行给定任务。

* **何时使用**：当期望的输出格式或风格具体或不太常见时，One-shot 提示很有用。它为模型提供了一个具体的学习实例。与 zero-shot 相比，它可以提高需要特定结构或语气的任务的性能。
* **示例**：  
  Translate the following English sentences to Spanish:  
  English: 'Thank you.'  
  Spanish: 'Gracias.'

  English: 'Please.'  
  Spanish:

## Few-Shot 提示

Few-shot 提示通过提供几个示例（通常是三到五个）的输入-输出对来增强 one-shot 提示。这旨在展示更清晰的预期响应模式，提高模型为新输入复制此模式的可能性。这种方法提供多个示例来引导模型遵循特定的输出模式。

* **何时使用**：Few-shot 提示对于需要遵循特定格式、风格或表现细微变化的任务特别有效。它非常适合分类、具有特定模式的数据提取或以特定风格生成文本等任务，尤其是当 zero-shot 或 one-shot 不能产生一致结果时。使用至少三到五个示例是一般经验法则，根据任务复杂性和模型 token 限制进行调整。
* **示例质量和多样性的重要性**：Few-shot 提示的有效性在很大程度上依赖于所提供示例的质量和多样性。示例应该准确、代表任务，并涵盖模型可能遇到的潜在变化或边缘情况。高质量、编写良好的示例至关重要；即使是一个小错误也可能混淆模型并导致不期望的输出。包含多样化的示例有助于模型更好地泛化到未见过的输入。
* **在分类示例中混合类别**：当使用 few-shot 提示进行分类任务（模型需要将输入分类到预定义类别）时，混合来自不同类别的示例顺序是最佳实践。这防止模型可能过度拟合特定的示例序列，并确保它学会独立识别每个类别的关键特征，从而在未见数据上实现更强大和可泛化的性能。
* **演变为"Many-Shot"学习**：随着像 Gemini 这样的现代 LLM 在长上下文建模方面变得更强大，它们在利用"many-shot"学习方面变得高度有效。这意味着现在可以通过在提示中直接包含大量示例（有时甚至数百个）来实现复杂任务的最佳性能，允许模型学习更复杂的模式。
* **示例**：  
  Classify the sentiment of the following movie reviews as POSITIVE, NEUTRAL, or NEGATIVE:

  Review: "The acting was superb and the story was engaging."  
  Sentiment: POSITIVE

  Review: "It was okay, nothing special."  
  Sentiment: NEUTRAL

  Review: "I found the plot confusing and the characters unlikable."  
  Sentiment: NEGATIVE

  Review: "The visuals were stunning, but the dialogue was weak."  
  Sentiment:

理解何时应用 zero-shot、one-shot 和 few-shot 提示技术，以及精心制作和组织示例，对于提高 Agentic 系统的有效性至关重要。这些基本方法为各种提示策略奠定了基础。

# 构建提示词

除了提供示例的基本技术之外，构建提示词的方式在引导语言模型方面起着关键作用。构建涉及在提示中使用不同的部分或元素，以清晰有组织的方式提供不同类型的信息，如指令、上下文或示例。这有助于模型正确解析提示并理解每段文本的特定角色。

## 系统提示

系统提示为语言模型设置整体上下文和目的，定义其在交互或会话中的预期行为。这涉及提供建立规则、角色或整体行为的指令或背景信息。与特定的用户查询不同，系统提示为模型的响应提供基础指南。它影响模型在整个交互过程中的语气、风格和一般方法。例如，系统提示可以指示模型始终简洁有益地响应，或确保响应适合一般受众。系统提示还用于安全和毒性控制，包括保持尊重语言等指南。

此外，为了最大化其有效性，系统提示可以通过基于 LLM 的迭代改进进行自动提示优化。Vertex AI Prompt Optimizer 等服务通过根据用户定义的指标和目标数据系统地改进提示来促进这一点，确保给定任务的最高性能。

* **示例**：  
  You are a helpful and harmless AI assistant. Respond to all queries in a polite and informative manner. Do not generate content that is harmful, biased, or inappropriate

## 角色提示

角色提示为语言模型分配特定的角色、人格或身份，通常与系统或上下文提示结合使用。这涉及指示模型采用与该角色相关的知识、语气和沟通风格。例如，"Act as a travel guide"（作为旅游指南）或"You are an expert data analyst"（你是专家数据分析师）等提示引导模型反映该分配角色的视角和专业知识。定义角色为语气、风格和专注的专业知识提供框架，旨在提高输出的质量和相关性。还可以指定角色内的期望风格，例如"幽默和鼓舞人心的风格"。

* **示例**：  
  Act as a seasoned travel blogger. Write a short, engaging paragraph about the best hidden gem in Rome.

## 使用分隔符

有效的提示涉及为语言模型清晰区分指令、上下文、示例和输入。可以使用分隔符，如三重反引号（\`\`\`）、XML 标签（\<instruction\>、\<context\>）或标记（---），来在视觉和程序上分隔这些部分。这种在提示工程中广泛使用的做法，通过确保提示每个部分的角色清晰，最小化模型的误解。

* **示例**：  
  \<instruction\>Summarize the following article, focusing on the main arguments presented by the author.\</instruction\>  
  \<article\>  
  \[Insert the full text of the article here\]  
  \</article\>

# 上下文工程

上下文工程与静态系统提示不同，动态提供对任务和对话至关重要的背景信息。这种不断变化的信息帮助模型掌握细微差别、回忆过去的交互并整合相关细节，从而产生有根据的响应和更流畅的交流。示例包括先前的对话、相关文档（如在检索增强生成中）或特定的操作参数。例如，在讨论日本旅行时，可能会要求在东京提供三个适合家庭的活动，利用现有的对话上下文。在 Agentic 系统中，上下文工程是核心 Agent 行为（如内存持久性、决策制定和跨子任务协调）的基础。具有动态上下文管道的 Agent 可以随时间维持目标、调整策略，并与其他 Agent 或工具无缝协作——这些是长期自主性的基本品质。这种方法论认为，模型输出的质量更多地取决于所提供上下文的丰富性，而不是模型的架构。它标志着从传统提示工程的重大演变，传统提示工程主要关注优化直接用户查询的措辞。上下文工程将其范围扩展到包括多层信息。

这些层包括：

* **系统提示**：定义 AI 操作参数的基础指令（例如，"你是技术作家；你的语气必须正式和精确"）。
* **外部数据**：  
  * **检索的文档**：主动从知识库获取以告知响应的信息（例如，提取技术规格）。
  * **工具输出**：AI 使用外部 API 获取实时数据的结果（例如，查询日历以获取可用性）。
* **隐式数据**：关键信息，如用户身份、交互历史和环境状态。整合隐式上下文带来与隐私和道德数据管理相关的挑战。因此，强大的治理对于上下文工程至关重要，尤其是在企业、医疗保健和金融等领域。

核心原则是，即使是高级模型，如果对其操作环境的视图有限或构建不当，也会表现不佳。这种做法将任务从仅仅回答问题重新定义为为 Agent 构建全面的操作图景。例如，上下文工程的 Agent 会在响应查询之前整合用户的日历可用性（工具输出）、与电子邮件收件人的专业关系（隐式数据）以及先前会议的笔记（检索的文档）。这使模型能够生成高度相关、个性化和实用有用的输出。"工程"方面涉及创建强大的管道以在运行时获取和转换此数据，并建立反馈循环以持续提高上下文质量。

为了实现这一点，专门的调优系统（如 Google 的 Vertex AI prompt optimizer）可以大规模自动化改进过程。通过系统地根据样本输入和预定义的指标评估响应，这些工具可以提高模型性能，并在不同模型之间调整提示和系统指令，而无需大量手动重写。为优化器提供样本提示、系统指令和模板，使其能够程序化地改进上下文输入，为实施复杂上下文工程所需的反馈循环提供结构化方法。

这种结构化方法将基本的 AI 工具与更复杂的、具有上下文意识的系统区分开来。它将上下文视为主要组件，强调 Agent 知道什么、何时知道以及如何使用该信息。这种做法确保模型对用户的意图、历史和当前环境有全面的理解。最终，上下文工程是将无状态聊天机器人转变为高度能干的、情境感知系统的关键方法论。

# 结构化输出

通常，提示的目标不仅仅是获得自由形式的文本响应，而是以特定的、机器可读的格式提取或生成信息。请求结构化输出（如 JSON、XML、CSV 或 Markdown 表格）是一种关键的结构化技术。通过明确要求以特定格式输出并可能提供期望结构的模式或示例，您可以引导模型以一种可以被 Agentic 系统或应用的其他部分轻松解析和使用的方式组织其响应。返回 JSON 对象进行数据提取是有益的，因为它强制模型创建结构并可以限制幻觉。建议尝试输出格式，特别是对于非创意任务，如提取或分类数据。

* **示例**：  
  Extract the following information from the text below and return it as a JSON object with keys "name", "address", and "phone\_number".

  Text: "Contact John Smith at 123 Main St, Anytown, CA or call (555) 123-4567."

有效利用系统提示、角色分配、上下文信息、分隔符和结构化输出显著增强了与语言模型交互的清晰度、控制力和实用性，为开发可靠的 Agentic 系统提供了坚实的基础。请求结构化输出对于创建管道至关重要，其中语言模型的输出作为后续系统或处理步骤的输入。

**利用 Pydantic 实现面向对象的外观**：强制执行结构化输出和增强互操作性的强大技术是使用 LLM 生成的数据填充 Pydantic 对象的实例。Pydantic 是一个使用 Python 类型注解进行数据验证和设置管理的 Python 库。通过定义 Pydantic 模型，您可以为期望的数据结构创建清晰且可强制执行的模式。这种方法有效地为提示的输出提供了面向对象的外观，将原始文本或半结构化数据转换为经过验证的、类型提示的 Python 对象。

您可以使用 model\_validate\_json 方法直接将来自 LLM 的 JSON 字符串解析为 Pydantic 对象。这特别有用，因为它在一个步骤中结合了解析和验证。

```python
from pydantic import BaseModel, EmailStr, Field, ValidationError
from typing import List, Optional
from datetime import date

# --- Pydantic 模型定义（来自上面）---
class User(BaseModel):
    name: str = Field(..., description="用户的全名。")
    email: EmailStr = Field(..., description="用户的电子邮件地址。")
    date_of_birth: Optional[date] = Field(None, description="用户的出生日期。")
    interests: List[str] = Field(default_factory=list, description="用户兴趣列表。")

# --- 假设的 LLM 输出 ---
llm_output_json = """
{
    "name": "Alice Wonderland",
    "email": "alice.w@example.com",
    "date_of_birth": "1995-07-21",
    "interests": [
        "Natural Language Processing",
        "Python Programming",
        "Gardening"
    ]
}
"""

# --- 解析和验证 ---
try:
    # 使用 model_validate_json 类方法解析 JSON 字符串。
    # 这一步解析 JSON 并根据 User 模型验证数据。
    user_object = User.model_validate_json(llm_output_json)

    # 现在你可以使用干净、类型安全的 Python 对象。
    print("成功创建 User 对象！")
    print(f"姓名：{user_object.name}")
    print(f"电子邮件：{user_object.email}")
    print(f"出生日期：{user_object.date_of_birth}")
    print(f"第一个兴趣：{user_object.interests[0]}")
    # 您可以像任何其他 Python 对象属性一样访问数据。
    # Pydantic 已将 'date_of_birth' 字符串转换为 datetime.date 对象。
    print(f"date_of_birth 的类型：{type(user_object.date_of_birth)}")

except ValidationError as e:
    # 如果 JSON 格式错误或数据与模型的类型不匹配，
    # Pydantic 将引发 ValidationError。
    print("无法验证来自 LLM 的 JSON。")
    print(e)
```

这段 Python 代码演示了如何使用 Pydantic 库定义数据模型和验证 JSON 数据。它定义了一个 User 模型，其中包含姓名、电子邮件、出生日期和兴趣字段，包括类型提示和描述。然后，代码使用 User 模型的 model\_validate\_json 方法解析来自大型语言模型（LLM）的假设 JSON 输出。此方法根据模型的结构和类型处理 JSON 解析和数据验证。最后，代码从结果 Python 对象访问验证的数据，并包括 ValidationError 的错误处理，以防 JSON 无效。

对于 XML 数据，可以使用 xmltodict 库将 XML 转换为字典，然后可以将其传递给 Pydantic 模型进行解析。通过在 Pydantic 模型中使用 Field 别名，您可以无缝地将 XML 的通常冗长或属性密集的结构映射到对象的字段。

这种方法对于确保基于 LLM 的组件与更大系统的其他部分的互操作性非常宝贵。当 LLM 的输出封装在 Pydantic 对象中时，可以可靠地将其传递给其他函数、API 或数据处理管道，并确保数据符合预期的结构和类型。这种在系统组件边界"解析，而不是验证"的做法导致更强大和可维护的应用。

有效利用系统提示、角色分配、上下文信息、分隔符和结构化输出显著增强了与语言模型交互的清晰度、控制力和实用性，为开发可靠的 Agentic 系统提供了坚实的基础。请求结构化输出对于创建管道至关重要，其中语言模型的输出作为后续系统或处理步骤的输入。

构建提示词 除了提供示例的基本技术之外，构建提示词的方式在引导语言模型方面起着关键作用。构建涉及在提示中使用不同的部分或元素，以清晰有组织的方式提供不同类型的信息，如指令、上下文或示例。这有助于模型正确解析提示并理解每段文本的特定角色。

# 推理和思维过程技术

大型语言模型擅长模式识别和文本生成，但在需要复杂的、多步骤推理的任务中通常面临挑战。本附录重点介绍旨在通过鼓励模型揭示其内部思维过程来增强这些推理能力的技术。具体来说，它解决了改进逻辑推演、数学计算和规划的方法。

## 思维链（CoT）

思维链（CoT）提示技术是一种强大的方法，通过明确提示模型在得出最终答案之前生成中间推理步骤来提高语言模型的推理能力。您不仅仅是要求结果，而是指示模型"逐步思考"。这个过程反映了人类可能如何将问题分解为更小、更易管理的部分并按顺序处理它们。

CoT 帮助 LLM 生成更准确的答案，特别是对于需要某种形式的计算或逻辑推演的任务，在这些任务中，模型可能会遇到困难并产生错误结果。通过生成这些中间步骤，模型更有可能保持正轨并正确执行必要的操作。

CoT 有两个主要变体：

* **Zero-Shot CoT**：这涉及简单地在提示中添加短语"Let's think step by step"（让我们逐步思考）（或类似措辞），而不提供任何推理过程的示例。令人惊讶的是，对于许多任务，这个简单的添加可以通过触发其暴露内部推理轨迹的能力来显著提高模型的性能。
  * **示例（Zero-Shot CoT）**：  
    If a train travels at 60 miles per hour and covers a distance of 240 miles, how long did the journey take? Let's think step by step.

* **Few-Shot CoT**：这将 CoT 与 few-shot 提示结合起来。您向模型提供几个示例，其中显示了输入、逐步推理过程和最终输出。这为模型提供了更清晰的模板，说明如何执行推理和构建其响应，与 zero-shot CoT 相比，通常在更复杂的任务上产生更好的结果。
  * **示例（Few-Shot CoT）**：  
    Q: The sum of three consecutive integers is 36\. What are the integers?  
    A: Let the first integer be x. The next consecutive integer is x+1, and the third is x+2. The sum is x \+ (x+1) \+ (x+2) \= 3x \+ 3\. We know the sum is 36, so 3x \+ 3 \= 36\. Subtract 3 from both sides: 3x \= 33\. Divide by 3: x \= 11\. The integers are 11, 11+1=12, and 11+2=13. The integers are 11, 12, and 13\.

    Q: Sarah has 5 apples, and she buys 8 more. She eats 3 apples. How many apples does she have left? Let's think step by step.  
    A: Let's think step by step. Sarah starts with 5 apples. She buys 8 more, so she adds 8 to her initial amount: 5 \+ 8 \= 13 apples. Then, she eats 3 apples, so we subtract 3 from the total: 13 \- 3 \= 10\. Sarah has 10 apples left. The answer is 10\.

CoT 提供了几个优势。它实施起来相对容易，并且可以在现成的 LLM 上非常有效，无需微调。一个重要的好处是模型输出的可解释性增加；您可以看到它遵循的推理步骤，这有助于理解为什么它得出特定答案，以及在出现问题时进行调试。此外，CoT 似乎提高了提示在不同版本的语言模型之间的鲁棒性，这意味着当模型更新时性能不太可能下降。主要缺点是生成推理步骤会增加输出的长度，导致更高的 token 使用量，这可能会增加成本和响应时间。

CoT 的最佳实践包括确保最终答案在推理步骤*之后*呈现，因为推理的生成影响答案的后续 token 预测。此外，对于具有单一正确答案的任务（如数学问题），建议在使用 CoT 时将模型的温度设置为 0（贪婪解码），以确保在每一步确定性地选择最可能的下一个 token。

## 自我一致性

基于思维链的思想，自我一致性技术旨在通过利用语言模型的概率性质来提高推理的可靠性。自我一致性不依赖于单一的贪婪推理路径（如基本 CoT），而是为同一问题生成多个不同的推理路径，然后从中选择最一致的答案。

自我一致性涉及三个主要步骤：

1. **生成不同的推理路径**：同一提示（通常是 CoT 提示）被多次发送到 LLM。通过使用更高的温度设置，鼓励模型探索不同的推理方法并生成各种逐步解释。
2. **提取答案**：从每个生成的推理路径中提取最终答案。
3. **选择最常见的答案**：对提取的答案进行多数投票。在不同推理路径中出现最频繁的答案被选为最终的、最一致的答案。

这种方法提高了响应的准确性和连贯性，特别是对于可能存在多个有效推理路径或模型在单次尝试中可能容易出错的任务。好处是答案正确的伪概率可能性，从而提高整体准确性。然而，显著的代价是需要为同一查询多次运行模型，导致计算和费用大大增加。

* **示例（概念性）**：  
  * *提示*："陈述'所有鸟类都能飞'是真还是假？解释你的推理。"
  * *模型运行 1（高温度）*：推理大多数鸟类会飞，结论为真。
  * *模型运行 2（高温度）*：推理企鹅和鸵鸟，结论为假。
  * *模型运行 3（高温度）*：推理鸟类*一般*，简要提及例外，结论为真。
  * *自我一致性结果*：基于多数投票（真出现两次），最终答案是"真"。（注意：更复杂的方法会权衡推理质量）。

## 后退提示

后退提示通过首先要求语言模型考虑与任务相关的一般原则或概念，然后再处理具体细节来增强推理。对这个更广泛问题的响应然后用作解决原始问题的上下文。

这个过程允许语言模型激活相关的背景知识和更广泛的推理策略。通过关注基本原则或更高层次的抽象，模型可以生成更准确和有洞察力的答案，较少受表面元素的影响。最初考虑一般因素可以为生成特定的创意输出提供更强的基础。后退提示鼓励批判性思维和知识应用，通过强调一般原则可能减轻偏见。

* **示例**：  
  * *提示 1（后退）*："是什么关键因素造就了一个好的侦探故事？"
  * *模型响应 1*：（列出诸如红鲱鱼、令人信服的动机、有缺陷的主角、逻辑线索、令人满意的解决方案等元素）。
  * *提示 2（原始任务 + 后退上下文）*："使用好的侦探故事的关键因素\[在此插入模型响应 1\]，为一部设定在小镇的新神秘小说写一个简短的情节摘要。"

## 思维树（ToT）

思维树（ToT）是一种高级推理技术，扩展了思维链方法。它使语言模型能够同时探索多个推理路径，而不是遵循单一的线性进展。这种技术利用树结构，其中每个节点代表一个"思维"——作为中间步骤的连贯语言序列。从每个节点，模型可以分支出去，探索替代的推理路线。

ToT 特别适合需要探索、回溯或在得出解决方案之前评估多种可能性的复杂问题。虽然比线性思维链方法在计算上更苛刻且更复杂，但 ToT 可以在需要深思熟虑和探索性问题解决的任务上取得优异成果。它允许 Agent 考虑不同的视角，并通过调查"思维树"中的替代分支可能从最初的错误中恢复。

* **示例（概念性）**：对于像"基于这些情节点为故事开发三个不同的可能结局"这样的复杂创意写作任务，ToT 将允许模型从关键转折点探索不同的叙事分支，而不仅仅是生成一个线性延续。

这些推理和思维过程技术对于构建能够处理超越简单信息检索或文本生成的任务的 Agent 至关重要。通过提示模型暴露其推理、考虑多个视角或后退到一般原则，我们可以显著增强它们在 Agentic 系统中执行复杂认知任务的能力。

# 行动和交互技术

智能 Agent 具有主动参与环境的能力，超越生成文本。这包括利用工具、执行外部函数以及参与观察、推理和行动的迭代循环。本节研究旨在实现这些主动行为的提示技术。

## 工具使用/函数调用

Agent 的一个关键能力是使用外部工具或调用函数来执行超出其内部能力的操作。这些操作可能包括网络搜索、数据库访问、发送电子邮件、执行计算或与外部 API 交互。有效的工具使用提示涉及设计提示，指示模型适当的时机和方法来利用工具。

现代语言模型通常经过"函数调用"或"工具使用"的微调。这使它们能够解释可用工具的描述，包括其目的和参数。在收到用户请求后，模型可以确定工具使用的必要性，识别适当的工具，并格式化其调用所需的参数。模型不直接执行工具。相反，它生成结构化输出，通常为 JSON 格式，指定工具及其参数。然后，Agentic 系统处理此输出，执行工具，并将工具的结果提供回模型，将其集成到正在进行的交互中。

* **示例**：  
  You have access to a weather tool that can get the current weather for a specified city. The tool is called 'get\_current\_weather' and takes a 'city' parameter (string).

  User: What's the weather like in London right now?

  * *预期的模型输出（函数调用）*：  
    {  
      "tool\_code": "get\_current\_weather",  
      "tool\_name": "get\_current\_weather",  
      "parameters": {  
        "city": "London"  
      }  
    }

## ReAct（推理与行动）

ReAct 是 Reason and Act 的缩写，是一种结合思维链风格推理与使用工具执行操作的交错方式的提示范式。ReAct 模仿人类的操作方式——我们口头推理并采取行动来收集更多信息或朝着目标前进。

ReAct 模式涉及一个循环：

1. **思考**：模型生成思维过程，解释其当前理解和计划。
2. **行动**：基于思考，模型决定执行一个操作，通常使用工具（例如，搜索、计算器、API 调用）。它输出工具的名称和所需的输入。
3. **观察**：Agentic 系统执行工具并将结果（观察）提供回模型。
4. 循环继续，模型基于最新的"观察"生成新的"思考"，导致进一步的"行动"和"观察"，直到任务完成，模型输出"最终答案"。

这种思考和行动的交错过程允许 Agent 动态收集信息、对工具输出做出反应并改进其方法，使其特别有效于需要与动态环境或外部知识源交互的任务。

* **示例（说明性 ReAct 轨迹）**：  
  User: What is the capital of France and its current population?

  Thought: The user is asking for two pieces of information: the capital of France and its population. I can use a search tool to find this information. I will start by confirming the capital, although I am likely to know it.  
  Action: Search({"query": "capital of France"})  
  Observation: Paris

  Thought: Okay, the capital is Paris. Now I need to find the population of Paris.  
  Action: Search({"query": "current population of Paris"})  
  Observation: The population of Paris was estimated to be around 2.1 million people in 2023\.

  Thought: I have found both pieces of information. I can now provide the final answer to the user.  
  Final Answer: The capital of France is Paris, and its estimated population in 2023 was around 2.1 million people.

这些技术对于构建可以主动参与世界、检索实时信息并执行需要与外部系统交互的任务的 Agent 至关重要。

# 高级技术

除了基础、结构和推理模式之外，还有几种其他提示技术可以进一步增强 Agentic 系统的能力和效率。这些范围从使用 AI 优化提示到整合外部知识和根据用户特征定制响应。

## 自动提示工程（APE）

认识到制作有效的提示可能是一个复杂的迭代过程，自动提示工程（APE）探索使用语言模型本身来生成、评估和改进提示。这种方法旨在自动化提示编写过程，可能在不需要大量人工努力进行提示设计的情况下提高模型性能。

一般思想是拥有一个"元模型"或一个过程，该过程接受任务描述并生成多个候选提示。然后根据它们在给定输入集上产生的输出质量评估这些提示（可能使用 BLEU 或 ROUGE 等指标，或人工评估）。可以选择表现最好的提示，可能进一步改进，并用于目标任务。使用 LLM 生成用户查询的变体以训练聊天机器人是一个例子。

* **示例（概念性）**：开发人员提供描述："我需要一个可以从电子邮件中提取日期和发件人的提示。"APE 系统生成几个候选提示。这些在样本电子邮件上测试，选择始终提取正确信息的提示。

当然。这是使用 DSPy 等框架对程序化提示优化的重新措辞和略微扩展的解释：

另一种强大的提示优化技术，特别是由 DSPy 框架推广，涉及将提示视为可以自动优化的程序化模块，而不是静态文本。这种方法超越了手动试错，进入了更系统的、数据驱动的方法论。

这种技术的核心依赖于两个关键组件：

1. **金标准集（或高质量数据集）**：这是一组代表性的高质量输入-输出对。它作为"真实标准"，定义了给定任务的成功响应是什么样的。
2. **目标函数（或评分指标）**：这是一个自动评估 LLM 输出与数据集中对应"黄金"输出的函数。它返回一个分数，指示响应的质量、准确性或正确性。

使用这些组件，优化器（如贝叶斯优化器）系统地改进提示。这个过程通常涉及两个主要策略，可以独立使用或协同使用：

* **Few-Shot 示例优化**：优化器不是由开发人员手动选择 few-shot 提示的示例，而是从金标准集中程序化地采样不同的示例组合。然后它测试这些组合，以识别最有效地引导模型生成期望输出的特定示例集。

* **指令提示优化**：在这种方法中，优化器自动改进提示的核心指令。它使用 LLM 作为"元模型"来迭代地变异和重新措辞提示的文本——调整措辞、语气或结构——以发现哪种措辞从目标函数获得最高分数。

两种策略的最终目标是最大化目标函数的分数，有效地"训练"提示以产生与高质量金标准集持续接近的结果。通过结合这两种方法，系统可以同时优化*给模型什么指令*和*向它展示哪些示例*，导致为特定任务机器优化的高度有效和强大的提示。

## 迭代提示/改进

这种技术涉及从简单的基本提示开始，然后根据模型的初始响应迭代改进它。如果模型的输出不太正确，您分析不足之处并修改提示以解决它们。这不太关于自动化过程（如 APE），更多关于人工驱动的迭代设计循环。

* **示例**：  
  * *尝试 1*："为新型咖啡机写产品描述。"（结果太通用）。
  * *尝试 2*："为新型咖啡机写产品描述。突出其速度和清洁便利性。"（结果更好，但缺乏细节）。
  * *尝试 3*："为'SpeedClean Coffee Pro'写产品描述。强调其在不到 2 分钟内冲泡一壶咖啡的能力及其自清洁循环。针对忙碌的专业人士。"（结果更接近期望）。

## 提供负面示例

虽然"指令优于约束"的原则通常成立，但在某些情况下提供负面示例可能有帮助，尽管要谨慎使用。负面示例向模型显示输入和*不期望的*输出，或输入和*不应该*生成的输出。这可以帮助阐明边界或防止特定类型的错误响应。

* **示例**：  
  Generate a list of popular tourist attractions in Paris. Do NOT include the Eiffel Tower.

  Example of what NOT to do:  
  Input: List popular landmarks in Paris.  
  Output: The Eiffel Tower, The Louvre, Notre Dame Cathedral.

## 使用类比

使用类比来框定任务有时可以通过将其与熟悉的事物联系起来来帮助模型理解期望的输出或过程。这对于创意任务或解释复杂角色特别有用。

* **示例**：  
  Act as a "data chef". Take the raw ingredients (data points) and prepare a "summary dish" (report) that highlights the key flavors (trends) for a business audience.

## 因式认知/分解

对于非常复杂的任务，将整体目标分解为更小的、更易管理的子任务并对每个子任务分别提示模型可能是有效的。然后将子任务的结果组合以实现最终结果。这与提示链和规划相关，但强调问题的深思熟虑的分解。

* **示例**：写一篇研究论文：  
  * 提示 1："为一篇关于 AI 对就业市场影响的论文生成详细大纲。"
  * 提示 2："根据此大纲写介绍部分：\[插入大纲介绍\]。"
  * 提示 3："根据此大纲写'对白领工作的影响'部分：\[插入大纲部分\]。"（对其他部分重复）。
  * 提示 N："组合这些部分并写结论。"

## 检索增强生成（RAG）

RAG 是一种强大的技术，通过在提示过程中让语言模型访问外部的、最新的或领域特定的信息来增强语言模型。当用户提出问题时，系统首先从知识库（例如，数据库、一组文档、网络）检索相关文档或数据。然后将此检索的信息作为上下文包含在提示中，允许语言模型生成基于该外部知识的响应。这减轻了幻觉等问题，并提供对模型未训练的或非常新的信息的访问。这是需要处理动态或专有信息的 Agentic 系统的关键模式。

* **示例**：  
  * *用户查询*："Python 库'X'最新版本的新功能是什么？"
  * *系统操作*：在文档数据库中搜索"Python 库 X 最新功能"。
  * *对 LLM 的提示*："根据以下文档片段：\[插入检索的文本\]，解释 Python 库'X'最新版本的新功能。"

## 人格模式（用户人格）

虽然角色提示为*模型*分配角色，但人格模式涉及描述用户或模型输出的目标受众。这有助于模型在语言、复杂性、语气和提供的信息类型方面定制其响应。

* **示例**：  
  You are explaining quantum physics. The target audience is a high school student with no prior knowledge of the subject. Explain it simply and use analogies they might understand.

  Explain quantum physics: \[Insert basic explanation request\]

这些高级和补充技术为提示工程师提供了进一步的工具，以优化模型行为、整合外部信息，并为 Agentic 工作流中的特定用户和任务定制交互。

# 使用 Google Gems

Google 的 AI"Gems"（见图 1）代表其大型语言模型架构中的用户可配置功能。每个"Gem"作为核心 Gemini AI 的专门实例运行，为特定的可重复任务量身定制。用户通过提供一组明确的指令来创建 Gem，这建立了其操作参数。这个初始指令集定义了 Gem 的指定目的、响应风格和知识领域。底层模型被设计为在整个对话中始终遵守这些预定义的指令。

这允许为专注的应用创建高度专业化的 AI Agent。例如，可以配置 Gem 作为仅引用特定编程库的代码解释器。另一个可以被指示分析数据集，生成摘要而不进行推测性评论。不同的 Gem 可能作为遵守特定正式风格指南的翻译器。这个过程为人工智能创建持久的、特定于任务的上下文。

因此，用户避免了需要在每个新查询中重新建立相同的上下文信息。这种方法减少了对话冗余并提高了任务执行的效率。产生的交互更加专注，产生与用户初始要求始终一致的输出。这个框架允许对通用 AI 模型应用细粒度的、持久的用户指导。最终，Gems 实现了从通用交互到专业化、预定义的 AI 功能的转变。

![][image1]  
图 1：Google Gem 使用示例。

# 使用 LLM 改进提示（元方法）

我们已经探索了许多制作有效提示的技术，强调清晰性、结构以及提供上下文或示例。然而，这个过程可能是迭代的，有时具有挑战性。如果我们可以利用大型语言模型（如 Gemini）的强大能力来帮助我们*改进*我们的提示呢？这就是使用 LLM 进行提示改进的本质——一种"元"应用，其中 AI 协助优化给 AI 的指令。

这种能力特别"酷"，因为它代表了一种 AI 自我改进的形式，或至少是 AI 辅助人类改进与 AI 交互的形式。我们不是仅仅依赖人类直觉和试错，而是可以利用 LLM 对语言、模式甚至常见提示陷阱的理解来获得改进提示的建议。它将 LLM 转变为提示工程过程中的协作
伙伴。

这种元级提示方法的好处包括：

* **加速迭代**：比纯手动试错更快地获得改进建议。
* **识别盲点**：LLM 可能会发现您忽略的提示中的歧义或潜在误解。
* **学习机会**：通过查看 LLM 提出的建议类型，您可以更多地了解什么使提示有效，并提高您自己的提示工程技能。
* **可扩展性**：可能自动化提示优化过程的部分，特别是在处理大量提示时。

重要的是要注意，LLM 的建议并不总是完美的，应该像任何手动设计的提示一样进行评估和测试。然而，它提供了一个强大的起点，可以显著简化改进过程。

* **改进提示的示例**：  
  Analyze the following prompt for a language model and suggest ways to improve it to consistently extract the main topic and key entities (people, organizations, locations) from news articles. The current prompt sometimes misses entities or gets the main topic wrong.

  Existing Prompt:  
  "Summarize the main points and list important names and places from this article: \[insert article text\]"

  Suggestions for Improvement:

在这个例子中，我们使用 LLM 来批评和增强另一个提示。这种元级交互展示了这些模型的灵活性和强大功能，允许我们通过首先优化给它们的基本指令来构建更有效的 Agentic 系统。这是一个迷人的循环，其中 AI 帮助我们更好地与 AI 对话。

# 特定任务的提示

虽然到目前为止讨论的技术具有广泛的适用性，但某些任务受益于特定的提示考虑。这些在代码和多模态输入领域特别相关。

## 代码提示

语言模型，尤其是那些在大型代码数据集上训练的模型，可以成为开发人员的强大助手。代码提示涉及使用 LLM 来生成、解释、翻译或调试代码。存在各种用例：

* **编写代码的提示**：要求模型根据所需功能的描述生成代码片段或函数。
  * **示例**："编写一个 Python 函数，接受数字列表并返回平均值。"
* **解释代码的提示**：提供代码片段并要求模型逐行或摘要地解释它的作用。
  * **示例**："解释以下 JavaScript 代码片段：\[插入代码\]。"
* **翻译代码的提示**：要求模型将代码从一种编程语言翻译为另一种。
  * **示例**："将以下 Java 代码翻译为 C++：\[插入代码\]。"
* **调试和审查代码的提示**：提供有错误或可以改进的代码，并要求模型识别问题、建议修复或提供重构建议。
  * **示例**："以下 Python 代码给出'NameError'。有什么问题以及如何修复？\[插入代码和回溯\]。"

有效的代码提示通常需要提供足够的上下文、指定所需的语言和版本，并清楚地说明功能或问题。

## 多模态提示

虽然本附录的重点和当前大多数 LLM 交互都是基于文本的，但该领域正在迅速向能够跨不同模态（文本、图像、音频、视频等）处理和生成信息的多模态模型发展。多模态提示涉及使用输入的组合来引导模型。这指的是使用多种输入格式而不仅仅是文本。

* **示例**：提供图表图像并要求模型解释图表中显示的过程（图像输入 + 文本提示）。或提供图像并要求模型生成描述性标题（图像输入 + 文本提示 -> 文本输出）。

随着多模态能力变得更加复杂，提示技术将演变为有效利用这些组合的输入和输出。

# 最佳实践和实验

成为熟练的提示工程师是一个涉及持续学习和实验的迭代过程。值得重申和强调几个有价值的最佳实践：

* **提供示例**：提供 one-shot 或 few-shot 示例是引导模型的最有效方法之一。
* **简单设计**：保持提示简洁、清晰且易于理解。避免不必要的行话或过于复杂的措辞。
* **明确输出**：清楚地定义模型响应的期望格式、长度、风格和内容。
* **指令优于约束**：专注于告诉模型你想让它做什么，而不是你不想让它做什么。
* **控制最大 Token 长度**：使用模型配置或明确的提示指令来管理生成输出的长度。
* **在提示中使用变量**：对于应用中使用的提示，使用变量使其动态和可重用，避免硬编码特定值。
* **尝试输入格式和写作风格**：尝试不同的提示措辞方式（问题、陈述、指令）并尝试不同的语气或风格，看看什么能产生最好的结果。
* **对于 Few-Shot 分类任务提示，混合类别**：随机化来自不同类别的示例顺序，以防止过拟合。
* **适应模型更新**：语言模型不断更新。准备在新模型版本上测试现有提示，并调整它们以利用新功能或保持性能。
* **尝试输出格式**：特别是对于非创意任务，尝试请求结构化输出，如 JSON 或 XML。
* **与其他提示工程师一起实验**：与他人合作可以提供不同的视角并导致发现更有效的提示。
* **CoT 最佳实践**：记住思维链的具体实践，例如将答案放在推理之后，以及对于有单一正确答案的任务将温度设置为 0。
* **记录各种提示尝试**：这对于跟踪什么有效、什么无效以及为什么至关重要。维护提示、配置和结果的结构化记录。
* **在代码库中保存提示**：在将提示集成到应用中时，将它们存储在单独的、组织良好的文件中，以便更容易维护和版本控制。
* **依赖自动化测试和评估**：对于生产系统，实施自动化测试和评估程序以监控提示性能并确保对新数据的泛化。

提示工程是一项通过实践不断提高的技能。通过应用这些原则和技术，并保持对实验和文档的系统方法，您可以显著增强构建有效 Agentic 系统的能力。

# 结论

本附录提供了提示的全面概述，将其重新定位为一种有纪律的工程实践，而不是简单的提问行为。其核心目的是展示如何将通用语言模型转变为针对特定任务的专业化、可靠和高度能干的工具。旅程始于不可协商的核心原则，如清晰性、简洁性和迭代实验，这些是与 AI 有效沟通的基石。这些原则至关重要，因为它们减少了自然语言中固有的歧义，帮助引导模型的概率输出朝向单一正确的意图。在此基础上，基本技术（如 zero-shot、one-shot 和 few-shot 提示）作为通过示例展示预期行为的主要方法。这些方法提供不同级别的上下文指导，有力地塑造了模型的响应风格、语气和格式。除了示例之外，使用明确的角色、系统级指令和清晰的分隔符来构建提示为细粒度控制模型提供了必要的架构层。

这些技术的重要性在构建自主 Agent 的背景下变得至关重要，它们为复杂的多步骤操作提供了必要的控制和可靠性。为了让 Agent 有效地创建和执行计划，它必须利用高级推理模式，如思维链和思维树。这些复杂的方法迫使模型外化其逻辑步骤，系统地将复杂目标分解为一系列可管理的子任务。整个 Agentic 系统的运营可靠性取决于每个组件输出的可预测性。这正是为什么请求像 JSON 这样的结构化数据，并使用 Pydantic 等工具以编程方式验证它，不仅仅是一种便利，而是强大自动化的绝对必要条件。没有这种纪律，Agent 的内部认知组件无法可靠地通信，导致自动化工作流中的灾难性失败。最终，这些结构化和推理技术是成功地将模型的概率文本生成转换为 Agent 的确定性和可信的认知引擎的关键。

此外，这些提示是赋予 Agent 感知和作用于其环境的关键能力，弥合数字思维和现实世界交互之间的差距。ReAct 和原生函数调用等面向行动的框架是作为 Agent 双手的重要机制，允许它使用工具、查询 API 和操作数据。与此同时，检索增强生成（RAG）和更广泛的上下文工程学科作为 Agent 的感官功能。它们主动从外部知识库检索相关的实时信息，确保 Agent 的决策基于当前的事实现实。这一关键能力防止 Agent 在真空中运行，它将被限制在其静态且可能过时的训练数据中。因此，掌握这一完整的提示范围是将通用语言模型从简单的文本生成器提升为真正复杂的 Agent 的决定性技能，能够以自主性、意识和智能执行复杂任务。

# 参考文献

以下是进一步阅读和深入探索提示工程技术的资源列表：

1. Prompt Engineering, [https://www.kaggle.com/whitepaper-prompt-engineering](https://www://www.kaggle.com/whitepaper-prompt-engineering)   
2. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)   
3. Self-Consistency Improves Chain of Thought Reasoning in Language Models,  [https://arxiv.org/pdf/2203.11171](https://arxiv.org/pdf/2203.11171)   
4. ReAct: Synergizing Reasoning and Acting in Language Models, [https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)  
5. Tree of Thoughts: Deliberate Problem Solving with Large Language Models,  [https://arxiv.org/pdf/2305.10601](https://arxiv.org/pdf/2305.10601)   
6. Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models, [https://arxiv.org/abs/2310.06117](https://arxiv.org/abs/2310.06117)   
7. DSPy: Programming—not prompting—Foundation Models [https://github.com/stanfordnlp/dspy](https://github.com/stanfordnlp/dspy) 

[image1]: ../images/appendix-a/image1.png